# -*- coding: utf-8 -*-
"""01TextPrep.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14tCV1JosZiggBpwcsTqDd2ytyM7d3A5w

##**Preparación de Texto**

**Preparar notebook**
"""

## Conectar el notebook a googledrive
##from google.colab import drive
##drive.mount('/content/drive')

## Importar librerias necesarias
import nltk
import pandas as pd
import numpy as np
import re
import codecs
import matplotlib.pyplot as plt
from collections import Counter
from nltk.util import ngrams
from nltk import bigrams
import spacy

## Descargar el corpus de nltk para 'tokenizer', 'stopwords'
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Importar stopwords
from nltk.corpus import stopwords
stop_words_nltk = set(stopwords.words('spanish'))

from nltk.probability import ConditionalFreqDist
from nltk.tokenize import word_tokenize

# Descargar spaCy Language Model
!python -m spacy download es_core_news_lg
nlp = spacy.load("es_core_news_lg")

"""**Cargar los datos para el análisis**"""

df = pd.read_csv('/content/export.csv', encoding='latin-1')
print(df.shape)
df.head(2)

df.WORKTYPE.value_counts()

df.REPORTDATE = pd.to_datetime(df.REPORTDATE)

df.REPORTDATE.describe()

#Seleccionar las columnas de interés para el análisis
datos = df[['RESUMEN', 'WORKLOGID', 'DETALLE']]
#datos = df.loc[:100, ['RESUMEN', 'WORKLOGID', 'DETALLE']]
datos.head()

#Eliminar líneas sin datos en la columna "DETALLE"
datos = datos.dropna(subset = ['DETALLE']).reset_index(drop=True)
datos.shape

"""**Limpieza de texto**

Creación de funciones para la limpieza de texto
"""

# Definición de función para lematizar los tokens
def lemma_texto(texto):
  lemma = []
  for token in nlp(texto):
      lemma.append( token.lemma_ )
  text_new = ' '.join(map(str, lemma))
  return text_new

## Definición del listado de palabras vacías de nltk
w_vacias = list(stop_words_nltk)

##Defición de función para remover del listado de tokens stopwords y tokens de longitud menor que "n"
def remover_stopwords (tokens):
  n=2
  for i in range(len(tokens)):
    tokens[i] = [w for w in tokens[i] if w not in w_vacias and len(w)>n]
  return tokens

## Definición de la función para la limpieza del texto
def limpiar_texto (data):
  data['procesado'] = data['DETALLE'] # Crear columna para almacenar el texto procesado
  data['procesado'] = data['procesado'].apply(lambda x: re.sub('<[^>]*>', ' ', str(x))) #Remover Tags HTML
  data['procesado'] = data['procesado'].apply(lambda x: str(x).lower()) #Normalizar texto a minúscula
  data['procesado'] = data['procesado'].apply(lambda x: lemma_texto(x)) #Lematizacion
  data['procesado'] = data['procesado'].apply(lambda x: str(x).translate(str.maketrans('áéíóúüñ','aeiouun'))) #Cambiar tildes y ñ
  data['procesado'] = data['procesado'].apply(lambda x: re.sub('[^A-Za-z]+', ' ', str(x))) #Retirar todos los caracteres no letras
  #data['procesado']=data['procesado'].apply(lambda x: cambiar_sinonimos(x)) # aplicar la función cambiar_sinonimos
  data['tokens'] = data['procesado'].apply(lambda x: nltk.word_tokenize(str(x))) #Crear columna con los tokens
  data['tokens_proc'] = data['tokens'] # Crear columna para almacenar los tokens procesados
  data['tokens_proc'] = remover_stopwords(data['tokens_proc']) # Remover stopwords y tokens "cortos" del listado de tokens
  return data

"""Aplicar la función para limpieza de texto en la columna "DETALLE"

*   Normalizar
*   Lematizar
*   Remover números y caracteres especiales
*   Tokenizar
*   Remover "palabras vacías" (stopwords)


"""

# Limpiar el texto en la columna "DETALLE"
limpiar_texto(datos)

"""**Análisis descriptivo inicial de los textos**

Descriptivo de los textos
"""

# Calculo de la longitud de las cadenas de tokens (tokens en cada texto luego del preprocesamiento)
datos['longitud_texto'] = datos['tokens_proc'].apply(lambda x: len(x))

#Eliminamos aquellos registros que, luego de procesados, no contienen información en el campo de texto
datos = datos[datos['longitud_texto'] > 0]

datos.shape

datos.reset_index(inplace= True, drop=True)

datos.head()

#estadísticos asociados a la longitud de las cadenas de tokens
datos['longitud_texto'].describe()

plt.figure(figsize=(5,4))
plt.hist(datos['longitud_texto'], bins = 200)
plt.title('Distribución de frecuencia de la longitud de los textos')
plt.xlabel('Longitud del texto')
plt.ylabel('Cantidad de registros')
plt.show()

"""Se observa que hasta el $75\%$ de los textos tienen 45 tokens o menos"""

circles = dict(markerfacecolor='red', marker='o')
plt.figure(figsize=(8,4))
plt.boxplot(datos['longitud_texto'], vert=False, flierprops=circles)
plt.title('Longitud de los textos')
plt.xlabel('Cantidad de tokens del texto')
plt.show()

"""Descriptivo de los tokens y la bolsa de palabras (BoW)"""

## Obtener listado de tokens (ya procesados)
tokens = []
for t in datos['tokens_proc']:
    tokens.extend(t)
print('Cantidad total de tokens = ',str(len(tokens)))

## Distribución de frecuencia de los tokens: Tamaño de BoW

fdist_tokens = nltk.FreqDist(tokens) #Devuelve un diccionario con clave=token y valor=frecuencia.
print('Tamanno de BoW=',len(fdist_tokens))
print('\n')

#Convertir el diccionario en un dataframe para facilitar el procesamiento
bow_frecuencia = pd.DataFrame([[key, fdist_tokens[key]] for key in fdist_tokens.keys()], columns=['palabra', 'frecuencia'])

print(bow_frecuencia.describe())

plt.figure(figsize=(5,4))
plt.hist(bow_frecuencia['frecuencia'], bins = 200)
plt.title('Distribución de frecuencia de ocurrencia de las palabras')
plt.xlabel('Frecuencia de aparición')
plt.ylabel('Cantidad de palabras')
plt.yscale('log')
plt.show()

"""Se observa que del total de 16025 tokens presentes en el BoW, hasta el $75\%$ de ellas aparecen 8 veces o menos."""

circles = dict(markerfacecolor='red', marker='o')
plt.figure(figsize=(8,4))
plt.boxplot(bow_frecuencia['frecuencia'], vert=False, flierprops=circles)
plt.title('Ocurrencia de palabras')
plt.xlabel('Cantidad de ocurrencias por palabra')
plt.xscale("log")
plt.show()

#Listado de "n" Tokens más comunes
n = 50
top_tokens = fdist_tokens.most_common(n)
x,y = zip(*top_tokens)
plt.figure(figsize=(14,4))
plt.bar(x,y)
plt.xticks(rotation=90)
plt.show()

#Extraer el BoW para procesamiento posterior
BoW = pd.DataFrame([[key, fdist_tokens[key]] for key in fdist_tokens.keys()], columns=['palabra', 'frecuencia'])
BoW = BoW.sort_values(by = 'palabra').reset_index(drop=True)
print(BoW.shape)
BoW.head()

"""Eliminar palabras de poca relevancia o valor"""

##Defición de función que reciba la columna del dataframe con los tokens y una lista de palabras para eliminar
def remover_palabra (tokens):
  for i in range(len(tokens)):
    tokens[i] = [w for w in tokens[i] if w not in palabras_vacias]
  return tokens

# Carga de listado de otras palabras vacías
## Nombres propios

w_vacias = pd.read_csv('/content/w_vacias.csv')
palabras_vacias = w_vacias['Palabra'].values.tolist()

w_vacias

datos['tokens_proc'] = remover_palabra(datos['tokens_proc'])

"""Revisar el nuevo resultado"""

## Obtener listado de tokens (ya procesados)
tokens_new = []
for t in datos['tokens_proc']:
    tokens_new.extend(t)
print('Cantidad total de tokens = ',str(len(tokens_new)))

## Distribución de frecuencia de los tokens
fdist_tokens_new = nltk.FreqDist(tokens_new) #Devuelve un diccionario con clave=token y valor=frecuencia.
print('Tamanno de BoW=',len(fdist_tokens_new))
print('\n')

#Convertir el diccionario en un dataframe para facilitar el procesamiento
bow_frecuencia_new = pd.DataFrame([[key, fdist_tokens_new[key]] for key in fdist_tokens_new.keys()], columns=['palabra', 'frecuencia'])

print(bow_frecuencia_new.describe())

plt.figure(figsize=(5,4))
plt.hist(bow_frecuencia_new['frecuencia'], bins = 200)
plt.title('Distribución de frecuencia de ocurrencia de las palabras')
plt.xlabel('Frecuencia de aparición')
plt.ylabel('Cantidad de palabras')
plt.yscale('log')
plt.show()

circles = dict(markerfacecolor='red', marker='o')
plt.figure(figsize=(8,4))
plt.boxplot(bow_frecuencia_new['frecuencia'], vert=False, flierprops=circles)
plt.title('Ocurrencia de palabras')
plt.xlabel('Cantidad de ocurrencias por palabra')
plt.xscale("log")
plt.show()

#Listado de "n" Tokens más comunes
n = 30
top_tokens = fdist_tokens_new.most_common(n)
x,y = zip(*top_tokens)
plt.figure(figsize=(4,5.5))
plt.barh(x, y)
plt.gca().invert_yaxis()
##plt.tight_layout()
#plt.xticks(rotation=90)
plt.show()

"""Bigramas"""

## Distribución de frecuencia de los bigramas.
fdist_bigrams = nltk.FreqDist(bigrams(tokens_new))

#Listado de "n" bigramas más comunes
m = 30
top_bigrams = fdist_bigrams.most_common(m)
top_bigrams

# Invierte el orden de las listas para que las barras sean horizontales
palabras = [' '.join(bigrama[0]) for bigrama in top_bigrams]
frecuencias = [bigrama[1] for bigrama in top_bigrams]

# Crea un rango para el eje y basado en la longitud de la lista de palabras
y = range(len(palabras))

# Crea el gráfico de barras horizontales
plt.figure(figsize=(5,6.5 ))  # Ajusta el tamaño de la figura para barras horizontales
plt.barh(y, frecuencias)  # Usa barh en lugar de bar para crear barras horizontales
#plt.xlabel('Frecuencia de Aparición')  # Etiqueta del eje x
#plt.ylabel('Bigramas')  # Etiqueta del eje y
plt.yticks(y, palabras)  # Etiquetas personalizadas para el eje y
plt.gca().invert_yaxis()
#plt.title('Frecuencia de Aparición de Bigramas')  # Título del gráfico
##plt.tight_layout()  # Ajusta el diseño del gráfico
plt.show()  # Muestra el gráfico

## Distribución de frecuencia de los trigramas.
fdist_trigrams = nltk.FreqDist(ngrams(tokens_new,3))
print('Tamanno del listado de trigramas=',len(fdist_trigrams))

#Listado de "n" trigramas más comunes
top_trigrams_pend = fdist_trigrams.most_common(30)
top_trigrams_pend

"""Exportar los datos para ser usados en el análisis"""

#datos.to_csv('/content/datos.csv', encoding='latin-1')