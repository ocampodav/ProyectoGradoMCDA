# -*- coding: utf-8 -*-
"""03ReduccionDimModelado_Unigram.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14bGKrLNPJbYvwrf3Sv0rEnQKcdmemLNM

**Preparar notebook**
"""

## Conectar el notebook a googledrive
#from google.colab import drive
#drive.mount('/content/drive')

## Importar librerias necesarias
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt

"""**Cargar los Datos Preparados**"""

df = pd.read_csv('/content/datos.csv', encoding='latin-1')
df.shape

## Recuperar los tokes (como lista) que se extrajeron en el textprep y que al exportar el archivo .csv se cargaron como cadenas de texto
df['tokens_proc'] = df['tokens_proc'].apply(lambda x: re.sub('[\[\]\']+', '', str(x)))
df['tokens_proc'] = df['tokens_proc'].apply(lambda x: x.split(', '))

df=df[['WORKLOGID','DETALLE', 'tokens_proc']]

df

df_tfidf = pd.read_csv('/content/tfidf.csv', encoding='latin-1')
df_tfidf.head()

df_tfidf.drop(['Unnamed: 0'], axis = 1, inplace=True)

df_tfidf

"""# Reducción de dimensionalidad

**PCA**
"""

from sklearn.decomposition import PCA

"""Análisis de componentes principales sobre representación vectorial de bigramas"""

n_componentes = 10
pca = PCA(n_components=n_componentes)
PrincipalComponents = pca.fit_transform(df_tfidf)

componentes = pca.fit(df_tfidf).components_

print(np.round(pca.explained_variance_ratio_, 6))
plt.figure(figsize = (4, 4))
plt.bar(np.arange(n_componentes), pca.explained_variance_ratio_)
plt.title("Varianza explicada por las componentes")
plt.show()

var_acum = np.cumsum(pca.explained_variance_ratio_)
print(var_acum)
plt.figure(figsize = (4, 4))
plt.plot(var_acum)
plt.title("Varianza acumulada de las componentes principales")
plt.show()

componentes = pd.DataFrame(componentes).transpose()
componentes.columns = pca.get_feature_names_out()
componentes.index =  df_tfidf.columns
print(componentes)

componentes['pca0'].sort_values(ascending = False)

principalDf = pd.DataFrame(data = PrincipalComponents)

principalDf

x = principalDf[0]
y = principalDf[1]
z = principalDf[2]
plt.figure(figsize = (5, 5))
plot_axes = plt.axes(projection = '3d')
plot_axes.scatter3D(x, y, z)
plot_axes.set_xlabel('Componente 1')
plot_axes.set_ylabel('Componente 2')
plot_axes.set_zlabel('Componente 3')
plt.show()

"""# Modelado

**kmeans**
"""

from sklearn.cluster import KMeans
from sklearn import metrics
from sklearn.metrics import pairwise_distances_argmin_min

"""Crear el modelo sobre las componentes principales"""

ks = range(1, 20)
inertias = []

for k in ks:
    # Crear  modelo
    model = KMeans(n_clusters=k,  n_init = 10, max_iter=500,)
    model.fit(principalDf)
    inertias.append(model.inertia_)

# Graficar cantidad de clusters vs inertias
plt.plot(ks, inertias, '-o')
plt.xlabel('Numero de clusters, k')
plt.ylabel('inertia')
plt.title("Inercia Vs Número de Clusters")
plt.xticks(ks)
plt.show()

k=10
kmeans_pca = KMeans(n_clusters=k, max_iter=500, n_init = 10)
kmeans_pca.fit(principalDf)

print('Inercia= {}'.format(kmeans_pca.inertia_))
print('Silueta= {}'. format(metrics.silhouette_score(principalDf, kmeans_pca.labels_)))

x = principalDf[0]
y = principalDf[1]
z = principalDf[2]

color = ['gray', 'blue','red','green','purple','cyan','yellow', 'black', 'orange', 'olive', 'pink', 'violet']
c=[]
for i in kmeans_pca.labels_:
  c.append(color[i])

plt.figure(figsize = (5, 5))
plot_axes = plt.axes(projection = '3d')
plot_axes.scatter3D(x, y, z, c=c)

plot_axes.set_xlabel('Componente 1')
plot_axes.set_ylabel('Componente 2')
plot_axes.set_zlabel('Componente 3')

plt.figure(figsize = (4, 4))
plt.scatter(x, z, c=c)
plt.xlabel('Componente 1')
plt.ylabel('Componente 2')
plt.show()

plt.figure(figsize = (4, 4))
plt.scatter(x, y, c=c)
plt.xlabel('Componente 1')
plt.ylabel('Componente 3')
plt.show()

plt.figure(figsize = (4, 4))
plt.scatter(y, z, c=c)
plt.xlabel('Componente 2')
plt.ylabel('Componente 3')
plt.show()

"""Centroides de los clusters"""

centroides=pd.DataFrame(kmeans_pca.cluster_centers_, columns=principalDf.columns.values)
centroides

#vemos el representante del grupo, el usuario cercano a su centroid
mas_cercano, _ = pairwise_distances_argmin_min(kmeans_pca.cluster_centers_, principalDf)
mas_cercano

"""Asignar etiquetas de los datos"""

df['label'] = kmeans_pca.labels_
df.head()

"""Cantidad de registros por cluster"""

df['label'].value_counts()

df.iloc[mas_cercano]

for i in mas_cercano:
  serie = df_tfidf.iloc[i]
  print(serie[serie > 0].index.values)