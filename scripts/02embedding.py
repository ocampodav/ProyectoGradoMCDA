# -*- coding: utf-8 -*-
"""02Embedding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zm8ALsaw4B0FYwtXwKHzs28CDx0nhigb

**Preparar notebook**
"""

## Conectar el notebook a googledrive
from google.colab import drive
drive.mount('/content/drive')

## Importar librerias necesarias
import nltk
import pandas as pd
import numpy as np
import re
import codecs
import matplotlib.pyplot as plt
from collections import Counter
from nltk.util import ngrams
from nltk import bigrams
import spacy

## Descargar el corpus de nltk para 'tokenizer', 'stopwords'
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Importar stopwords
from nltk.corpus import stopwords
stop_words_nltk = set(stopwords.words('spanish'))

from nltk.probability import ConditionalFreqDist
from nltk.tokenize import word_tokenize

# Descargar spaCy Language Model
!python -m spacy download es_core_news_lg
nlp = spacy.load("es_core_news_lg") #sm: no word vectors, md: reduced word vector, lg: large word vector, trf: transformer pipeline without static word vectors)

"""**Cargar los Datos Preparados**"""

df = pd.read_csv('/content/datos.csv', encoding='latin-1')
df.shape

## Recuperar los tokes (como lista) que se extrajeron en el textprep y que al exportar el archivo .csv se cargaron como cadenas de texto
df['tokens_proc'] = df['tokens_proc'].apply(lambda x: re.sub('[\[\]\']+', '', str(x)))
df['tokens_proc'] = df['tokens_proc'].apply(lambda x: x.split(', '))

df.head()

#Seleccionar las columnas de interés para el análisis
datos = df[['WORKLOGID','tokens_proc']]
datos.head(5)

"""## Representación de Documentos"""

## Representación de Documentos
doc = datos.copy()

# Definición para pasar una lista de token a un texto
def list_to_text (lista):
  text = ' '.join(lista)
  return text

#aplicamos la funcion list_to_text
doc['text'] = doc['tokens_proc'].apply(lambda x: list_to_text(x))

doc['text']

## Obtener listado de tokens (ya procesados)
tokens = []
for t in doc['tokens_proc']:
    tokens.extend(t)
print('Cantidad total de tokens = ',str(len(tokens)))
## Distribución de frecuencia de los tokens. Tamaño del BoW
fdist_tokens = nltk.FreqDist(tokens)
print('Tamanno del BoW=',len(fdist_tokens))

#Convertir el diccionario en un dataframe para facilitar el procesamiento
bow = pd.DataFrame([[key, fdist_tokens[key]] for key in fdist_tokens.keys()], columns=['palabra', 'frecuencia'])

print(bow.describe())

#se crea lista de textos
texts = doc['text'].tolist()

"""**TF-IDF Representation**"""

#Vectorización
from sklearn.feature_extraction.text import TfidfVectorizer

# using default tokenizer in TfidfVectorizer
tfidf = TfidfVectorizer(min_df=0.0005, max_df=0.95, ngram_range=(1, 1)) # eliminar aquellos que ocurren en muy pocos documentos (min_df = 0.5%)
                                                                        # eliminar aquellos que ocurren en demasiados documentos (max_df = 95%)
features_tfidf = tfidf.fit_transform(texts)
df_tfidf=pd.DataFrame(
    features_tfidf.todense(),
    columns=tfidf.get_feature_names_out()
)

df_tfidf

df_tfidf.to_csv('/content/tfidf.csv', encoding='latin-1')

# using default tokenizer in TfidfVectorizer
tfidf_bigram = TfidfVectorizer(min_df=0.0005, max_df=0.95, ngram_range=(2, 2))
features_tfidf_bigram = tfidf_bigram.fit_transform(texts)
df_tfidf_bigram=pd.DataFrame(
    features_tfidf_bigram.todense(),
    columns=tfidf_bigram.get_feature_names_out()
)

df_tfidf_bigram

df_tfidf_bigram.to_csv('/content/tfidf_bigram.csv', encoding='latin-1')