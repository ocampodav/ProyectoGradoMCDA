# -*- coding: utf-8 -*-
"""05NER_EntityRuler.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TfZ0Z6ZW5utghkVpSxOvAx5SD90jO0SV

#**Reconocimiento de entidades (NER) basado en reglas**##

[Language Processing Pipelines](https://spacy.io/usage/processing-pipelines)

####**Preparar notebook**
"""

## Conectar el notebook a googledrive
#from google.colab import drive
#drive.mount('/content/drive')

## Importar librerias necesarias
import nltk
import pandas as pd
import numpy as np
import re
import codecs
from collections import Counter
import spacy

print(spacy.__version__)

## Descargar el corpus de nltk para 'tokenizer', 'stopwords'
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Importar stopwords
from nltk.corpus import stopwords
stop_words_nltk = set(stopwords.words('spanish'))

from nltk.tokenize import word_tokenize

# Descargar spaCy Language Model
!python -m spacy download es_core_news_lg
#nlp = spacy.load("es_core_news_lg")

"""####**Cargar los datos para el análisis**"""

data = pd.read_csv('/content/export.csv', encoding='latin-1')
print(data.shape)
data.head(2)

#Seleccionar las columnas de interés para el análisis
df = data[['RESUMEN', 'WORKLOGID', 'DETALLE']]
df.head()

#Eliminar líneas sin datos en la columna "DETALLE"
df = df.dropna(subset = ['DETALLE']).reset_index(drop=True)
df.shape

"""####**Limpieza de texto**

Creación de funciones para la limpieza de texto
"""

## Definición del listado de palabras vacías de nltk
w_vacias = list(stop_words_nltk)

##Defición de función para remover del listado de tokens stopwords y tokens de longitud menor que "n"
def remover_stopwords (tokens):
  n=2
  for i in range(len(tokens)):
    tokens[i] = [w for w in tokens[i] if w not in w_vacias and len(w)>n]
  return tokens

## Definición de la función para la limpieza del texto
def limpiar_texto (data):
  data['procesado'] = data['DETALLE'] # Crear columna para almacenar el texto procesado
  data['procesado'] = data['procesado'].apply(lambda x: re.sub('<[^>]*>', ' ', str(x))) #Remover Tags HTML
  data['procesado'] = data['procesado'].apply(lambda x: str(x).lower()) #Normalizar texto a minúscula
  data['procesado'] = data['procesado'].apply(lambda x: str(x).translate(str.maketrans('áéíóúüñ','aeiouun'))) #Cambiar tildes y ñ
  data['procesado'] = data['procesado'].apply(lambda x: re.sub('[^A-Za-z]+', ' ', str(x))) #Retirar todos los caracteres no letras
  data['tokens'] = data['procesado'].apply(lambda x: nltk.word_tokenize(str(x))) #Crear columna con los tokens
  data['tokens_proc'] = data['tokens'] # Crear columna para almacenar los tokens procesados
  data['tokens_proc'] = remover_stopwords(data['tokens_proc']) # Remover stopwords y tokens "cortos" del listado de tokens
  return data

"""Aplicar la función para limpieza de texto en la columna "DETALLE"

*   Normalizar
*   Remover números y caracteres especiales
*   Tokenizar
*   Remover "palabras vacías" (stopwords)


"""

# Limpiar el texto en la columna "DETALLE"
limpiar_texto(df)
## se tarda unos 6min

"""Eliminar palabras de poca relevancia o valor"""

##Defición de función que reciba la columna del dataframe con los tokens y una lista de palabras para eliminar
def remover_palabra (tokens):
  for i in range(len(tokens)):
    tokens[i] = [w for w in tokens[i] if w not in palabras_vacias]
  return tokens

# Carga de listado de otras palabras vacías
## Nombres propios

w_vacias = pd.read_csv('/content/w_vacias.csv')
palabras_vacias = w_vacias['Palabra'].values.tolist()

df

df['tokens_proc'] = remover_palabra(df['tokens_proc'])

df['texto'] = df['tokens_proc'].apply(lambda x: ' '.join(x))

"""## **Ajustar el pipeline y aplicar el modelo NER**


[Rule-based matching](https://spacy.io/usage/rule-based-matching#matcher)

[Rule-based entity recognition](https://spacy.io/usage/rule-based-matching#entityruler)

####**Ajustar el pipeline y cargar los patrones de entidades desde un archivo**
"""

#desahibitar componentes que no se necesitan
nlp = spacy.load("es_core_news_lg", disable=['tok2vec',
                                      'morphologizer',
                                      'parser',
                                      'attribute_ruler',
                                      'lemmatizer',])

# add the entity ruler before the "ner" component. the entity recognizer will respect the existing entity spans and adjust its predictions around it.
ruler = nlp.add_pipe('entity_ruler', before='ner').from_disk('/content/drive/MyDrive/Master DS/ProyectoGrado/Datos/patrones.jsonl')

nlp.pipe_names

"""####**Aplicar el modelo NER**"""

docs = nlp.pipe(df['texto'].tolist())
entidades = []
for doc in docs:
    entidades_linea = [(ent.text, ent.label_) for ent in doc.ents]
    entidades.append(entidades_linea)

df['entidades'] = entidades

## Definir lista de ENTIDADES de interes ---
#entidad_interes = ['COMPONENTE', 'CAUSA']

## Definir función para extraer palabras de interes según TAG
#def ExtractInterest(text, entities):
#  interesting = [(k,v) for k,v in text if v in entities]
#  return(interesting)

"""####**Extraer las entidades de interés**"""

## Definir función para extraer palabras de interes según TAG - Causas
def ExtractCauses(text):
  causes = [k for k,v in text if v in 'CAUSA']
  return(list(set(causes)))

## Definir función para extraer palabras de interes según TAG - Componentes
def ExtractComponents(text):
  components = [k for k,v in text if v in 'COMPONENTE']
  return(list(set(components)))

df['causas'] = df['entidades'].apply(lambda x: ExtractCauses(x))
df['componentes'] = df['entidades'].apply(lambda x: ExtractComponents(x))

df.tail()

"""## **Explorar los resultados**"""

df['long_causas'] = df['causas'].apply(lambda x: len(x))
df['long_componentes'] = df['componentes'].apply(lambda x: len(x))
df['long_tokens'] = df['tokens_proc'].apply(lambda x: len(x))

df.head()

#muestra = df[['DETALLE','texto', 'causas', 'componentes']]
#muestra.to_csv('/content/drive/MyDrive/Master DS/ProyectoGrado/Datos/muestra.csv', index = False)

"""Un ejemplo de los resultados obtenidos"""

row = 13410

print(df.iloc[row]['DETALLE'])
print(df.iloc[row]['texto'])
print(df.iloc[row]['causas'])
print(df.iloc[row]['componentes'])

"""Identificar para cuantos registros fue posible identificar una al menos una entidad "CAUSA" y una entidad "COMPONENTE"
"""

# Contar filas donde 'longitud' sea mayor que cero (lista no vacía)
causas_identif= (df['long_causas'] > 0).sum()
componentes_identif = (df['long_componentes'] > 0).sum()
sin_info = ((df['long_componentes'] == 0)&(df['long_causas'] == 0)).sum()
causas_componente_identif = ((df['long_componentes'] > 0)&(df['long_causas'] > 0)).sum()


print("Cantidad de registros:", df.shape[0])
print("Número registros con causa identificada:", causas_identif)
print("Porcentaje registros con causa identificada:", np.round(causas_identif/df.shape[0]*100, 0),"%")
print("Número registros con componente identificado:", componentes_identif)
print("Porcentaje registros con componente identificado:", np.round(componentes_identif/df.shape[0]*100, 0),"%")
print("Número registros con causa y componente identificados:", causas_componente_identif)
print("Porcentaje registros con causa y componente identificados:", np.round(causas_componente_identif/df.shape[0]*100, 0),"%")
print("Número registros sin causa ni componente identificados:", sin_info)
print("Porcentaje registros sin causa ni componente identificados:", np.round(sin_info/df.shape[0]*100, 0),"%")

"""Realizar el mismo análisis anterior considerando solamente los registros donde el procesamiento de texto haya entregado algun resultado (cadena de tokens procesados no vacía)"""

registros_proc_novacio = (df['long_tokens'] > 0).sum()

print("Cantidad de registros:", str(registros_proc_novacio))
print("Porcentaje registros con causa identificada:", np.round(causas_identif/registros_proc_novacio*100, 2),"%")
print("Porcentaje registros con componente identificado:", np.round(componentes_identif/registros_proc_novacio*100, 2),"%")
print("Porcentaje registros sin causa ni componente identificados:", np.round(sin_info/registros_proc_novacio*100, 2),"%")

"""Craer un set de datos completo con los datos originales y los resultados del NER para su análisis"""

#Un dataframe con los resultados exlcusivamente
df = df.drop(['RESUMEN', 'DETALLE'], axis=1)

df.head()

data = data.merge(df, on = 'WORKLOGID')

data.head()

# Filtrar el DataFrame para obtener solo las filas donde 'longitud' es igual a cero
filas_longitud_cero = data[(data['long_causas'] == 0) & (data['long_componentes'] == 0)]

# Extraer la columna de texto 'procesado' de las filas filtradas
columna_texto_longitud_cero = filas_longitud_cero[['WORKLOGID', 'procesado']]

# Mostrar el dataframe resultante
#print(columna_texto_longitud_cero)
columna_texto_longitud_cero

print(data.iloc[8503]['componentes'])

"""####**Estimar del desempeño del modelo**

Se trata el resultado como una clasificación binaria, donde se evaluará si el modelo es capaz de extraer en forma correcta al menos una entidad, "CAUSA" o "COMPONENTE" para aquellos registros en donde estas entidades existan.

* Obtener una muestra aleatoria de los datos
"""

#crear una lista con un conjunto de k numeros enteros aleatorios entre 0 y el tamaño n del dataframe

n = df.shape[0]
k = 50

# Genera una lista de 50 números enteros aleatorios entre k y n
indices_aleatorios = np.random.randint(0, n, k)

# Muestra la lista resultante
#indices_aleatorios

#se guardó la lista de indices para hacer consistente la evaluación
indices = [ 3036,  1734,  6100,  9379,  5411,  7480, 12853,  2663,  2142,
         768,   211,  7785,   150,  4033,  9750, 13130,  4668,  3527,
        6825, 10779,  3934,  6125,  6676,  4517, 12031,  2851,  8243,
       12296, 12444, 13048,   389,  7289,  2595,  5719,  7340,  2355,
        7789,  2923, 12789,  9185, 10175, 12802,  2493,  7602,  4761,
        8308,  2175,  4516, 12268,  1055, 26,  7409,  3100, 13208,
       7031, 12096,  4574,  8036,  3998,
        1557,  7162,  5246,  4352, 10033,  5707, 10076,  4244,
       12191,  6995, 10428,  8252,   900,   400, 12069,  1784, 10115,
        2519,  4609, 13100,  5403,  3501, 11260,  2470,  3550,   371,
        7483,  2222,  8649,  1824, 10485,   854,  6651, 10928,  5703,
        7328, 10243,  7218,  3984,  5229,  8503]

#se obtiene una muestra aleatoria de los registros, con las variables de interés
muestra = data.iloc[indices][['WORKLOGID','DETALLE','texto', 'causas', 'componentes']]

muestra

"""Los resultados en la muestra se evalúan en forma manual, revisando uno a uno los registros, para comparar el resultado entregado por el modelo con la evaluación realizada con el criterio de personal técnico.
Se asigna un "1" para los casos en que existe al menos una entidad en el registro o un "0" en caso contrario
"""

evaluacion_df = pd.read_excel('/content/drive/MyDrive/Master DS/ProyectoGrado/Datos/evaluacion.xlsx', usecols = ['worklogid', 'causa_pred','causa_true','componentes_pred', 'componentes_true'] )

evaluacion_df

"""Se calculan alguna métricas sobre la clasificación binaria propuesta"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, ConfusionMatrixDisplay, confusion_matrix
import matplotlib.pyplot as plt

accuracy_causa = accuracy_score(evaluacion_df.causa_true, evaluacion_df.causa_pred)
 precision_causa = precision_score(evaluacion_df.causa_true, evaluacion_df.causa_pred)
 recall_causa = recall_score(evaluacion_df.causa_true, evaluacion_df.causa_pred)

 print("""métricas calculadas para la "predicción" de la entidad causa""")
 print("accuracy:", np.round(accuracy_causa,2) ,"; precision:", np.round(precision_causa,2), "; recall:", np.round(recall_causa,2))

accuracy_componente = accuracy_score(evaluacion_df.componentes_true, evaluacion_df.componentes_pred)
precision_componente = precision_score(evaluacion_df.componentes_true, evaluacion_df.componentes_pred)
recall_componente = recall_score(evaluacion_df.componentes_true, evaluacion_df.componentes_pred)

print("""métricas calculadas para la "predicción" de la entidad componente""")
print("accuracy:", np.round(accuracy_componente,2) ,"; precision:", np.round(precision_componente,2), "; recall:", np.round(recall_componente,2))

confusion_matrix(evaluacion_df.causa_true, evaluacion_df.causa_pred)

ConfusionMatrixDisplay.from_predictions(evaluacion_df.causa_true, evaluacion_df.causa_pred)
plt.show()

ConfusionMatrixDisplay.from_predictions(evaluacion_df.componentes_true, evaluacion_df.componentes_pred)
plt.show()

"""####**Aplicación de los resultados**

#####**Refinamiento del etiquetado de los datos**
"""

#Extraer los datos con codigos de falla "FAILURECODE" genéricos (electricos y mecánicos)
generic_df =  data[(data['FAILURECODE'] == 'MECGEN') | (data['FAILURECODE'] == 'ELECTGEN')].reset_index(drop = True)
generic_df.head()

generic_df = generic_df[generic_df['long_causas'] > 0][['WONUM', 'DESCRIPTION','procesado','FAILURECODE', 'causas','long_causas']].reset_index(drop = True)
generic_df.head()

# Obtén el máximo número de elementos en una lista de 'causas'
n_col_causa = generic_df['long_causas'].max()
n_col_causa

# Itera a través de las columnas 'causas' y crea nuevas columnas
for i in range(n_col_causa):
    generic_df[f'causa_{i+1}'] = generic_df['causas'].apply(lambda x: x[i] if i < len(x) else None)

# Elimina la columna original 'causas' si es necesario
generic_df.drop('causas', axis=1, inplace=True)

generic_df

"""#####**Identificación de fallas recurrentes (causas y componentes)**"""

#Extraer los datos con codigo de falla "FAILURECODE" MECGEN, Genérico mecánico
mecgen_df =  data[data['FAILURECODE'] == 'MECGEN'].reset_index(drop = True)
mecgen_df.head()

## Obtener listado de causas
causas_mecgen = []
for t in mecgen_df['causas']:
    causas_mecgen.extend(t)
print('Cantidad total de causas = ',str(len(causas_mecgen)))

## Obtener listado de componentes
component_mecgen = []
for t in mecgen_df['componentes']:
    component_mecgen.extend(t)
print('Cantidad total de componentes = ',str(len(component_mecgen)))

## Distribución de frecuencia de las causas

fdist_causas_mecgen = nltk.FreqDist(causas_mecgen) #Devuelve un diccionario con clave=causa y valor=frecuencia.
fdist_component_mecgen = nltk.FreqDist(component_mecgen) #Devuelve un diccionario con clave=componente y valor=frecuencia.

#Convertir el diccionario en un dataframe para facilitar el procesamiento
causas_mecgen_frecuencia = pd.DataFrame([[key, fdist_causas_mecgen[key]] for key in fdist_causas_mecgen.keys()], columns=['causa', 'frecuencia'])
component_mecgen_frecuencia = pd.DataFrame([[key, fdist_component_mecgen[key]] for key in fdist_component_mecgen.keys()], columns=['componente', 'frecuencia'])


print('Causas identificadas=',len(fdist_causas_mecgen))
#print(causas_mecgen_frecuencia.describe())
#print('\n')
print('Componentes identificados=',len(fdist_component_mecgen))
#print(component_mecgen_frecuencia.describe())

#Listado de "n" causas más comunes
n = 10
top_causas_mecgen = fdist_causas_mecgen.most_common(n)
x,y = zip(*top_causas_mecgen)
plt.figure(figsize=(4,3))
plt.barh(x,y)
plt.gca().invert_yaxis()
plt.xticks(rotation=90)
plt.show()

#Listado de "n" causas más comunes
top_component_mecgen = fdist_component_mecgen.most_common(n)
x,y = zip(*top_component_mecgen)
plt.figure(figsize=(4,3))
plt.barh(x,y)
plt.gca().invert_yaxis()
plt.xticks(rotation=90)
plt.show()

"""#####**Indicadores de confiabilidad**

En primer lugar identifiquemos algun sistema (registrado en el campo 'LOCATION') con una cantidad considerable registros de mantenimiento correctivo (atención de fallas)
"""

data.groupby('LOCATION')['WONUM'].count().sort_values(ascending=False).head(10)

"""Extraer un subconjunto de datos para realizar en análisis sobre un sistema, identificando un componente específico y la frecuencia de falla asociadas con dicho componente"""

#definir el sistema de interés
sistema = 'G3_CNV_AUXELEC_C/M'

#crear el subconjunto de datos
df_recurrente = data[(data['LOCATION']==sistema) & (data['long_componentes']>0)]
df_recurrente.head()

# Definir un componente de interés
componente = "cargador baterias"

# Filtra el DataFrame para que contenga solo registros con el valor específico
df_recurrente = df_recurrente[df_recurrente['componentes'].apply(lambda x: componente in x)]

df_recurrente = df_recurrente.drop_duplicates(subset='WONUM')
df_recurrente

# Convierte la columna 'REPORTDATE' al tipo de dato de fecha y hora
df_recurrente['REPORTDATE'] = pd.to_datetime(df_recurrente['REPORTDATE'], format='%d/%m/%Y %H:%M:%S')

df_recurrente = df_recurrente[['REPORTDATE', 'WONUM', 'DESCRIPTION','procesado','componentes']].sort_values('REPORTDATE')
df_recurrente

"""Calcular el tiempo promedio en que se presentan los eventos identificados, es decir, aquellos asociados al sistema cuyo código es G3_CNV_AUXELEC_C/M y donde se ha identificado "cargador baterias" como el componente con falla."""

# Calcula las diferencias en horas entre filas adyacentes
df_recurrente['diferencia_horas'] = df_recurrente['REPORTDATE'].diff().dt.total_seconds() / 3600

# Calcula el tiempo medio entre los eventos (fallas)
mtbf = df_recurrente['diferencia_horas'].sum()/len(df_recurrente)

# Muestra el DataFrame resultante y el promedio de las diferencias
print(df_recurrente['diferencia_horas'])
print("tiempo promedio entre eventos:", mtbf)